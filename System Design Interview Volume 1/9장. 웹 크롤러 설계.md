- 웹 크롤러는 로봇이나 스파이더라고 표현되기도 함
- 검색 엔진에서 널리쓰는 기술 중 하나로 **웹에 새로 올라오거나 갱신된 컨텐츠를 찾는 목적**으로 사용
- 사용처
	- **검색 엔진 인덱싱**: 웹 페이지를 모아 검색 엔진을 위한 로컬 인덱스 생성
	- **웹 아카이빙**: 나중에 사용할 목적으로 웹 페이지 장기보관을 위해 정보를 모으는 절차
	- **웹 마이닝**: 마이닝을 통한 지식 도출
	- **웹 모니터링**: 저작권이나 상표권 침해 사례 모니터링
- 처리해야 하는 데이터의 규모에 따라 복잡도의 차이 발생
## 문제 이해 및 설계 범위 확정
- 웹 크롤러의 기본 알고리즘
	1. URL 집합이 입력으로 주어지면, 해당 URL의 모든 웹 페이지 다운로드
	2. 다운받은 웹 페이지에서 URL 추출
	3. 추출된 URL들을 다운로드할 URL 목록에 추가하고 과정 반복
- 요구 사항을 분석해 설계 범위를 좁히는 과정 필요
	- 규모 확장성, 안정성, 예절, 확장성 고려 필요

## 개략적 설계안 제시 및 동의 구하기
![](https://velog.velcdn.com/images/kyy00n/post/7b970245-ec85-4421-b3f1-d4809034737d/image.png)
### 시작 URL 집합
- 웹 크롤러의 크롤링 시작점
- 전체 웹 크롤링의 경우 시작 URL 선택에 대한 고려 필요
	- 크롤러가 가능한 많은 링크를 탐색할 수 있도록 하는 URL 선택 필요
	- 일반적으로는 전체 URL 공간을 작은 부분집합으로 나누는 전략 사용
### 미수집 URL 저장소
- 크롤링 상태 구분
	1. 다운로드**할** URL
	2. 다운로드**된** URL
- 이 중 **다운로드 할 URL** 상태를 저장하는 컴포넌트
	- FIFO의 Queue
### HTML 다운로더
- 웹 페이지 다운로드 컴포넌트
### 도메인 이름 변환기
- 웹 페이지 다운을 위한 URL → IP 변환 절차
### 콘텐츠 파서
- 다운로드 한 웹 페이지 파싱 및 검증
	- 크롤링 서버 내에 파서를 구현하면 크롤링에 딜레이 발생 가능성
### 중복 컨텐츠?
- 공개된 연구 결과에 따르면 29% 가량의 웹 페이지 콘텐츠는 중복
- 자료구조 도입을 통한 데이터 중복 제거 및 소요 시간 감소
	- 해싱을 활용한 비교
### 콘텐츠 저장소
- HTML 문서 보관 시스템
- 적절한 저장소 고려 필요
	- 대부분의 컨텐츠는 디스크에 저장
	- 접근이 많은 컨텐츠는 메모리에 저장
### URL 추출기
- HTML 페이지를 파싱해 URL 링크 추출
### URL 필터
- 목적에 의한 필터 역할
	- 특정 컨텐츠 타입이나 파일 확장자를 갖는 URL
	- 접근 불가 URL
	- **접근 제외 목록**에 포함된 URL
### 이미 방문한 URL?
- **이미 방문한 URL 저장소**와 **미수집 URL 저장소**를 구현해 URL 상태 추적이 가능한 자료구조 구현
- 블룸 필터나 해시 테이블 활용
### URL 저장소
- 이미 방문한 URL 저장소
### 웹 크롤러 작업 흐름
1. 시작 URL들을 미수집 URL 저장소에 저장
2. HTML 다운로더는 미수집 URL 저장소에서 URL 목록 조회
3. HTML 다운로더는 도메인 이름 변환기를 사용해 URL의 IP 주소 확인 및, 해당 IP 웹 페이지 다운
4. 컨텐츠 파서는 다운로드된 HTML 페이지를 파싱 및 형식 검증
5. 컨텐츠 파싱 및 검증 이후 중복 컨텐츠 확인 절차 개시
6. 중복 컨텐츠 확인을 위해 저장소 조회
    - 이미 저장소에 있는 컨텐츠인 경우 제거
    - 저장소에 없는 컨텐츠인 경우 저장소에 저장후 URL 추출기 전달
7. URL 추출기로 해당 HTMl 페이지에서 링크 추출
8. 추출된 링크 URL 필터로 전달
9. 필터링 이후 남은 URL만 중복 URL 판별 단계로 전달
10. URL 저장소에 보관한 URL인지 판단 후 처리
11. 저장소에 없는 URL은 URL 저장소에 저장 이후 미수집 URL 저장소에도 전달

## 상세 설계
### DFS vs BFS
- 웹은 **유향 그래프**와 같음
	- 페이지 = 노드 / 하이퍼링크 = 에지
	- 크롤링은 그래프를 탐색하는 과정과 같음
- DFS는 그래프 크기가 클 경우 깊이 가늠이 힘들어 좋은 선택이 아님
- 보통 BFS 사용
	- 한 페이지에서 나오는 링크의 상당 수는 같은 서버로 돌아감 → politeness 위배
	- 우선 순위의 부재 → 우선순위 고려 필요
### 미수집 URL 저장소
- BFS 문제 해결 가능
- politness나 freshness 구별 크롤러 구현 가능
#### 예의 (politeness)
- 동일 웹사이트에 대해 한번에 한 페이지만 요청
	- 웹 사이트 호스트 명과 다운로드를 수행하는 작업 스레드 관계 유지로 구현 가능
	- 각 다운로드 스레드는 별도의 FIFO Queue 활용 URL 다운로드
- URL 기반 Queue 분리를 진행하는 **Queue 라우터** 구현
	- 매핑 테이블에 기반해 Queue 분리
	- Queue들을 순회하면서 URL을 꺼내 작업 스레드에 전달하는 **Queue 선택기** 구현
	- 작업 스레드를 통해 URL 다운로드
#### 우선순위
- 유용성 기반 URL 우선순위 구분 시 페이지 랭크, 트래픽 양, 갱신 빈도와 같은 척도 사용 필요
#### 신선도
- 웹 페이지의 신선함을 유지하기 위해 다운로드한 페이지도 재수집에 의한 갱신이 필요
	- 웹 페이지 변경 이력 활용
	- 우선순위 기반 중요 페이지 재수집
#### 미수집 URL 저장소를 위한 지속성 저장장치
- URL을 모두 메모리에는 저장할 수 없음 → 메모리 버퍼 Queue 활용 디스크 기록 방식 적용
### HTML 다운로더
#### Robots.txt
- 로봇 제외 프로토콜 → 웹 사이트가 크롤러와 소통하는 표준
- 크롤러가 수집 가능/불가능 한 디렉토리 목록 표현
- 주기적으로 다시 다운받아 캐시에서 관리
#### 성능 최적화
1. 분산 크롤링: 성능 향상을 위해 크롤링 작업 분산
2. 도메인 이름 변환 결과 캐시:
	- DNS Resolve 과정이 동기적이라 병목 발생
	- 도메인 이름에 상응하는 IP 주소를 캐시에 보관, 주기적으로 갱신
3. 지역성: 크롤링 작업 수행 서버를 지역별로 분산
4. 짧은 타임아웃: 타임아웃 시간 지정을 통한 부하 관리
#### 안정성
- 안정 해시: 다운로드 서버 부하 분산을 위해 적용
- 크롤링 상태 및 수집 데이터 저장: 장애 발생시 복구를 위해 지속적 저장장치에 기록
- 예외 처리: 예외 발생시 시스템 중단이 발생하지 않도록 미리 처리
- 데이터 검증: 시스템 오류를 방지하기 위한 중요 수단
#### 확장성
- 모듈화를 활용해 새로운 컨텐츠 지원
#### 문제 있는 컨텐츠 감지 및 회피
1. 중복 컨텐츠
	- 웹 컨텐츠 중복을 피하기 위한 해시나 체크섬 활용
2. 거미 덫
	- 크롤러를 무한 루프에 빠트리도록 설계한 웹 페이지
	- 수작업으로 크롤러 탐색 대상에서 제외 및 필터 블랙 리스트 등록
3. 데이터 노이즈
	- 제외
## 마무리
- 좋은 크롤러의 특성
	- 규모 확장성
	- 예의
	- 확장성
	- 안정성
- 추가로 논의하면 좋을 것
	- 서버 측 렌더링
	- 원치 않는 페이지 필터링
	- 데이터베이스 다중화 및 샤딩
	- 수평적 규모 확장성
	- 가용성, 일관성, 안정성
	- 데이터 분석 솔루션